\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{enumerate}
\usepackage{relsize}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{multirow}

\usepackage{hyperref}
\usepackage[margin=1.5in]{geometry}
\hypersetup{colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue}

\usepackage{tikz}
\allowdisplaybreaks
\usetikzlibrary{graphs}
\usetikzlibrary{arrows.meta}

\usepackage{notations}
\graphicspath{{./figures/}}

\algrenewcommand\Require{\State \textbf{Require: }}
\newcommand{\Data}{\State \textbf{Data: }}
\newcommand{\Output}{\State \textbf{Output: }}

%\numberwithin{equation}{section}

\newcommand{\bZmu}{\widebar{Z\mu}}
\newcommand{\alg}{\text{alg}}

%%%%%%%%% Theorems
\theoremstyle{plain}
\newtheorem{prop}{Proposition}

\newtheorem{conj}[prop]{Conjecture}
\newtheorem{coro}[prop]{Corollary}
\newtheorem{lemm}[prop]{Lemma}
\newtheorem{theo}[prop]{Theorem}

\theoremstyle{definition}
\newtheorem{exam}{Example}
\newtheorem{defi}{Definition}
\newtheorem{assu}{Assumption}
\newtheorem{proo}{Proof}
\newtheorem{model}{Model}

\theoremstyle{remark}
\newtheorem{comm}{Comment}
\newtheorem{rema}{Remark}

\title{}
\author{}

\date{Draft version \ifcase\month\or
January\or February\or March\or April\or May\or June\or
July\or August\or September\or October\or November\or December\fi \ \number%
\year\ \  }


\begin{document}

\section{Single-Agent Markovian Persuasion Models}
 
There is a hidden Markov state $S_t \in \mathcal S$ whose transition follows the
time-homogeneous kernel
$S_{t+1} \sim P(\cdot \mid S_t = s, A_t = a)$.
The principal (sender) fully observes $S_t$ and commits to a signaling or recommendation policy $\pi$ that maps the state to a recommendation $R_t = \pi(S_t)$. The agent (receiver) does not observe $S_t$.  

At each time $t$, the agent hold a belief $\mu_t \in \Delta(\mathcal S)$ about $S_t$, observe the recommendation $R_t$, and then choose an action $A_t$. The agent’s one-period payoff is given by a function $u(a,s)$ for $a \in \mathcal A$, $s \in \mathcal S$,
and the principal’s one-period payoff is $v(A_t,S_t)$.
Let $H_t$ denote the history observed by the agent up to time $t$ (e.g., past recommendations and realized outcomes), which induces the belief
$\mu_t(\cdot) = \PP{S_t \in \cdot \cond H_t}$ under the announced policy. Consider the following three models that differ in how the agent uses information over time:
\begin{itemize}
\item The agent at time $t$ is a new, short-lived receiver who cares only about the current period.  Their prior is a fixed distribution $\mu$ that does not depend on $t$ or on $H_t$ (so effectively $\mu_t \equiv \mu$), and they choose
\begin{equation}
A_t \in \arg\max_{a \in \mathcal A} \EE[\mu]{v(a,S_t)\cond R_t}
\end{equation}
that maximizes only their current expected payoff. They do not track
information over time and do not care about future rewards.  This
corresponds to the standard ``short-lived receiver'' dynamic persuasion setup \citep{wu2022sequential}.
\item The same agent is present over all periods and updates their belief $\mu_t$ from history: $\mu_t(\cdot) = \PP{S_t \in \cdot \cond H_t}$. However, at each time $t$ they still choose $A_t$ to maximize only the current expected payoff, i.e.,
\begin{equation}
A_t \in \arg\max_{a \in \mathcal A} \EE{v(a,S_t)\cond H_t, R_t},
\end{equation}
while how current action affects future information or future payoffs.
In this sense the agent may track information over time (their belief $\mu_t$ changes with $H_t$), but their behavior is myopic: they never take actions purely to learn. This captures a long-lived agent who learns passively but does not engage in strategic exploration \citep{renault2017optimal,iyer2023markov,lehrer2021markovian}.
\item The same agent is present over all periods, forms a belief
$\mu_t(\cdot) = \PP{S_t \in \cdot \cond H_t}$, and cares about a discounted stream of payoffs
\begin{equation}
\EE{\sum_{k=t}^\infty \beta^{k-t} v(A_k,S_t)\cond H_t, R_t},
\end{equation}
for some $\beta \in (0,1)$, where the expectation is taken under the probability law induced by the
transition kernel $P$, the principal’s signaling policy $\pi$, and the agent’s strategy.
In this case it can be optimal for the agent to take actions that are suboptimal in the current period in order to improve future
information and explore deliberately.
\end{itemize}

\newpage
\section{Off-Policy Learning with Endogenous Belief}

Consider an experiment in which the experimenter (principal) runs a micro-randomized
experiment on a single user (agent) and collects a sequence of data
\((S_t,R_t,Y_t)_{t\ge1}\) under an experimental recommendation policy \(\pi_0\).
For example, on a short-video platform the experimenter can randomize how
videos are highlighted to the user; in an online advertisement campaign, the experimenter can randomize how promotion emails are sent on each day.  The goal is to find a target recommendation policy
\(\pi : \mathcal S \to \Delta(\mathcal R)\)  that optimizes the long-run average outcome for the principal.

A common approach is to model the system as a (time-homogeneous) MDP: 
conditionally on $(S_t,R_t)$, the next state is drawn as
$S_{t+1} \sim P(\cdot \cond S_t = s, R_t = r)$.
Conventional off-policy evaluation methods then treat $R_t$ as the
action in this MDP and use importance weighting or Bellman
equations to evaluate the long-run value of a target recommendation policy $\pi$.

Consider the case where the user has a (potentially unobserved, misspecified, and endogenous) belief $\mu_t \in \Delta(\mathcal S)$ about the current state $S_t$.
Rather than always following the recommendation, the user first updates their belief after observing $R_t$, and then chooses an action that maximizes their own one-period utility $v(a,S_t)$.  In particular, if the
recommendation policy $\pi$ is public,\footnote{In practice, users may track only a coarsened state $S_t' \in \mathcal S'$ and a belief $\mu_t'$ over $\mathcal S'$, rather than a full posterior over $\mathcal S$. The formula below then applies to $(S_t',\mu_t')$ with $\pi$ and $\mu_{t-1}$ replaced by their induced versions on $\mathcal S'$.} the posterior over
$S_t$ given $R_t=r$ and prior $\mu_{t-1}$ is
\begin{equation}
\mu_{t}(s;r,\mu_{t-1}) = \PP{S_t=s \cond R_t=r,\mu_{t-1}}
 =\frac{\pi(r \cond s)\mu_{t-1}(s)}{\sum_{s'} \pi(r \cond s')\,\mu_{t-1}(s')}
 \label{eq:bayes_belief}
\end{equation}
The user then plays a myopic best response
\begin{equation}
a^*(\mu_t)
\in \argmax_{a \in \mathcal A}
\sum_{s\in\mathcal S} v(a,s)\,\mu_t(s)
\end{equation}

Denote the principal's one-period payoff as $u(A_t,S_t)$, which depends on the state $S_t$ and the user's action $A_t$ (but not directly on
$R_t$). As the users always update their belief according to the Bayes rule, the pair $(S_t,\mu_t)$ evolves as a time-homogeneous Markov chain:
given $(S_t,\mu_t)$, the recommendation $R_t$ is drawn from
$\pi(\cdot \cond S_t)$; the user responds with $A_t\sim a^*(R_t,\mu_t)$;
the next state $S_{t+1}$ are drawn from the $P(\cdot\cond S_t,A_t)$;
and the belief $\mu_{t+1}$ is updated deterministically from
$(\mu_t,R_t,A_t)$ according to the user's learning rule. We denote by
$d_\pi$ the stationary distribution of this Markov chain on
$\mathcal S \times \Delta(\mathcal S)$.
Under some regularity assumptions (ergodic to be added), the stationary distribution $d_\pi$ exists and is unique, and the long-run average payoff is well-defined and independent of the initial belief.

Given $d_\pi$, the principal's stationary objective under policy $\pi$ can be written as
\begin{equation}
\EE[\pi]{u(A_t,S_t)}
= \EE[(S_t,\mu_t)\sim d_\pi]{\EE[A_t\sim a^*(\mu_t)]{u(A_t,S_t)\cond S_t, \mu_t} },
\end{equation}
Thus, the principal's value under a policy $\pi$ depends not only on the transition kernel for $S_t$ but also on the endogenous belief process $\cb{\mu_t}$ and the user's best-response strategy. 
Unless we impose the strong assumption that, for every $(s,r)$, the induced conditional distribution of the user’s action $\PP{A_t=a \cond S_t=s, R_t=r}$ is invariant across recommendation policies $\pi$, all conventional MDP approaches that treat $R_t$ as the action break down.

\subsection{Identifying Policy Effect in Belief-Dependent Environments}

To see how the existence of endogenous belief process makes generic off-policy learning fail even when belief is observed, let's start by considering the identification of off-policy value in such a belief-dependent environment. A key challenge is that the transition of the state $P_{\pi}(\cdot\cond S_t, \mu_t, R_t)$ now depends on the policy $\pi$ due to the endogenous belief update:
\begin{equation*}
P_{\pi}(S_{t+1}, \mu_{t+1}\cond S_t, \mu_t, R_t)
= P_S(S_{t+1}\cond S_t, a^*(\mu_t))\cdot 
I\cb{\mu_{t+1}(s) = \frac{\pi(R_t\cond s)\mu_t(s)}{\sum_{s'}\pi(R_t\cond s')\mu_t(s') },\forall s }
\end{equation*}
As a result, the usual off-policy identification argument, which assumes a single policy-invariant environment and only reweights using the state–action occupancy, breaks down.  In particular, to recover the value of a target policy $\pi$ from data collected under a logging policy $\pi_0$, we would need that every belief transition induced by $\pi$ can also arise under $\pi_0$. Fixing a prior $\mu_t$ and a realized recommendation $R_t$, this requires the posterior under $\pi$ to match the posterior under $\pi_0$:
\begin{equation}
\frac{\pi(R_t\cond s)\mu_t(s)}{\sum_{s'}\pi(R_t\cond s')\mu_t(s') }
=
\frac{\pi_0(R_t\cond s)\mu_t(s)}{\sum_{s'}\pi_0(R_t\cond s')\mu_t(s') },\qquad\forall s.
\end{equation}
When $\mu_t$ is a belief over the full state space, this equality forces $\pi(r\mid s)=\pi_0(r\mid s)$.  In other words, generic off-policy identification in this belief-dependent environment would require an overlap condition that is much stronger than the usual state–action overlap and is almost always violated.

One way to weaken this requirement is to assume that the agent only tracks a coarsened state $B_t=g(S_t)$ and updates their belief on $B_t$ using aggregated likelihoods (so that the Bayes update depends on $\PP{R_t=r \cond B_t=b}$ rather than on $\PP{R_t=r\cond S_t=s}$).  In that case, off-policy identification is still only possible within a restricted class of target policies that are observationally equivalent at the level of the tracked state, in the sense that they induce the same likelihoods
$\PP{R_t=r\mid B_t=b}$ as the logging policy. Outside of such restricted classes, the policy dependence of the belief dynamics implies that standard MDP-based OPE methods, which rely on a policy-invariant transition kernel, are misspecified in this setting.




\newpage
\section{Networked Markovian Persuasion with Myopic Agents}

We now start with the first, simplest model and extend it to a population of agents on a network.  The Markov state is now a vector $S_t = (S_{1,t},\ldots,S_{n,t}) \in \mathcal S^n$,
where $S_{i,t}$ describes the local state of region $i = 1,\ldots,n$.  The regions are connected by a graph $G=(V,E)$; we
write $N(i)$ for the neighborhood of $i$ (e.g., $i$ together with
its one-hop neighbors on the graph). The state still evolves according to a time-homogeneous transition kernel $S_{t+1} \sim P(\cdot \cond S_t = s, A_t = a)$,
where $A_t = (A_{1,t},\ldots,A_{n,t})$ is the profile of actions taken by the agents in period $t$. We focus on the case with one representative agent per region and identify agent $i$ with region $i$. 
The agent’s one-period conditional expected utility in region $i$ is given by a function $v_i : \mathcal A_i \times \mathcal S \to \RR$, while the principal’s one-period conditional expected utility is the average utility across regions,
\begin{equation}
u(A_t,S_t) = \frac{1}{n} \sum_{i=1}^n v_i\p{A_{i,t}, S_{t}}.
\end{equation}
We focus on long-run performance in persistent systems such as revenue on ride-sharing platforms or disease control for epidemic mitigation. In such cases, the relevant KPIs are naturally per-period averages (e.g., average waiting time, match rate, or infection incidence). Accordingly, the principal evaluates a (stationary) recommendation policy $\pi$ by its stationary average utility
\begin{equation}
U(\pi) = \lim_{T\to\infty}\frac{1}{T}\EE[\pi]{u(A_t,S_t)}.
\end{equation}
Under mild regularity conditions (to be added), the limit exists and there is a stationary distribution $d_{\pi}$ over states. In that case, the objective can be written as
\begin{equation}
U(\pi) = \EE[S\sim d_{\pi}]{\EE[A\sim\pi]{u(A,S)\cond S}}.
\end{equation}
This formulation captures the steady performance of the system after transients wash out.

Throughout, the principal fully observes $S_t$ and commits to a (possibly randomized) recommendation policy $\pi: \mathcal S^n \to \Delta(\mathcal A_1 \times \cdots \times \mathcal A_n)$
so that in period $t$ a recommendation profile
$R_t = (R_{1,t},\ldots,R_{n,t}) \sim \pi(\cdot \cond S_t)$
is drawn and privately communicated to the agents.  We again interpret $R_{i,t}$ as a direct recommendation for the action of agent $i$. All agents share a common prior $\mu$ over $S_t$ and know the policy $\pi$,
but they do not observe $S_t$ or the recommendations sent to other agents.

At each time $t$, agent $i$ observes
their own recommendation $R_{i,t}$ and then chooses an action
$A_{i,t} \in \mathcal A_i$ to maximize their current expected payoff:
\begin{equation}
A_{i,t}\cond R_{i,t}=r_i \in \arg\max_{a_i \in \mathcal A_i}
  \EE[\mu]{u_i(a_i,S_t) \cond R_{i,t}=r_i },
\end{equation}
where the expectation is taken over the posterior distribution of $S_t$ given $R_{i,t}=r_i$.  
By Bayes’ rule, this posterior is
\begin{equation}
\PP[\mu]{S_t = s \cond R_{i,t}=r_i}
= \frac{\mu(s) \PP{R_{i,t}=r_i \cond S_t=s}}{\sum_{s' \in \mathcal S^n} \mu(s')\PP{R_{i,t}=r_i \cond S_t=s'}},
\end{equation}
where
\begin{equation}
\PP{R_{i,t}=r_i \cond S_t=s} = \sum_{r_{-i} \in \mathcal A_{-i}}\pi\p{(r_i,r_{-i}) \cond s }.
\end{equation}

Following the classic work of \cite{kamenica2011bayesian}, we describe feasible outcomes in terms of the induced distributions over states and recommendations.  
For a given prior $\mu$ on $S_t$, and for each region $i$, we define the persuasion set
$\mathcal P_i(\mu)$ as the set of probability measures
$P \in \Delta(\mathcal S^n \times \mathcal A_i)$ such that the marginal of $P$ on $\mathcal S^n$ is $\mu$, and for every $a_i \in \mathcal A_i$ with $P(R_i = a_i) > 0$,
\begin{equation}
\EE[\mu]{v_i(a_i,S_t) \cond R_{i,t} = a_i }
\ge \EE[\mu]{v_i(a_i',S_t) \cond R_{i,t} = a_i }
\end{equation}
for all $a_i' \in \mathcal A_i$.
It is well known that, with Bayesian rational agents, any signaling policy is outcome-equivalent to a direct recommendation policy characterized by some $P \in \mathcal P_i(\mu)$.
Hence it is without loss of generality to work directly with the
persuasion set.

A natural choice of prior $\mu$ is the stationary distribution $d_\pi$ of $S_t$ induced by the recommendation policy $\pi$. This mirrors the classic Bayesian persuasion framework, where the sender observes a realized state drawn from a common prior; the difference in our dynamic setting is that this common prior is itself determined by the policy $\pi$. 
We interpret this formulation as targeting long-run performance: once the system has mixed, agents have learned the steady-state distribution of the state under $\pi$, but in each period they don't observe the realized state.\footnote{Another interesting setting is when agents only observe their own realized local state, but not the states of their neighbors, so the principal’s signal nudges their beliefs about the global state of the system.} They are thus Bayesian with prior $d_\pi$ and update their beliefs about the current state using the principal's recommendation, exactly as in the static persuasion model. The policy learning problem can then be summarized as the following optimization problem of finding the optimal occupancy measure $\gamma(a,s) = d_\pi(s)\pi(a\cond s)$ that maximizes long-term performance subject to incentive constraints:
\begin{equation*}
\begin{split}
\max_{\gamma} &\sum_{a,s} \p{\frac{1}{n}\sum_i v_i(a_i,s)} \gamma(a,s)\\
\text{s.t.} &\sum_s v_i(a_i,s)\sum_{a_{-i}}\gamma((a_i,a_{-i}),s)
\ge \sum_s v_i(a_i',s)\sum_{a_{-i}}\gamma((a_i,a_{-i}),s),\\ 
&\qquad\qquad\qquad\qquad\qquad\qquad\forall i, \forall a_i', \forall a_i \text{ with } \sum_{s,a_{-i}}\gamma((a_i,a_{-i}),s)>0,\\
& \sum_{a,s} \gamma(a,s)=1,\qquad \gamma(a,s)\ge 0,\qquad \forall a,\forall s\\
& \sum_{a,s} \gamma(a,s)P(s'\cond s,a) = \sum_a \gamma(a,s'),\qquad\forall s'.
\end{split}
\end{equation*}

\iffalse
\begin{rema}
Questions to think about:
\begin{itemize}
\item What happens if the prior $\mu$ is not observed by the principal?  
In that case, it is unclear what the persuasion set is. (i) assume $\mu$ is common knowledge; (ii) consider a set of $\mu$ and discuss robust/ambiguity-set-type extensions; (iii) assume certain best-response model and do structural estimation
\item What if the prior $\mu_t$ varies over time (as in the second case in Section~1)? The persuasion set can still be defined in a similar way period by period, but finding the optimal policy becomes more complicated. might be straightforward to extend the analysis when $\mu_t$ is exogenous, but may require substantially more work when $\mu_t$ is endogenous and depends on past signals and actions.
\end{itemize}
\end{rema}
\fi

\subsection{Local v.s. Global Policies}

We impose the following locality assumption on rewards and dynamics. 
This assumption says that, conditional on the current local environment around a region $i$, neither the instantaneous payoff nor the law of $S_{i,t+1}$ depends on what happens in distant parts of the network.  
In particular, externalities and information propagate across the system only through the edges of $G$. This will potentially allow us to work with local posteriors over $S_{N(i),t}$ and to formulate the persuasion and incentive-compatibility constraints in terms of neighborhoods $N(i)$ rather than the full state $S_t$.  
Such locality assumptions are reasonable in many applications where interactions are predominantly spatial or geographic, for example, ride-sharing platforms where waiting times and prices in an area depend mainly on nearby demand and supply, or epidemic and diffusion models where the state of a location next period depends on its own and its neighbors' states (add some references here). 

\begin{assu}
For each region $i$ there exists a neighborhood $N(i) \subseteq V$ such that:
\begin{enumerate}
\item The reward of agents in region $i$ depends only on local state in $N(i)$. In other words, for all $s \in \mathcal S^n$,
\begin{equation}
v_i(a_i,s) = v_i(a_i,s_{N(i)}).
\end{equation}
\item The next-period state of region $i$ depends on $(S_t,A_t)$ only through the local configuration $(S_{N(i),t},A_{N(i),t})$.  Formally, for all measurable $\mathcal S' \subseteq \mathcal S$,
\begin{equation}
\PP{S_{i,t+1} \in \mathcal S'\cond S_t = s, A_t = a} = 
\PP{S_{i,t+1} \in \mathcal S'\cond S_t = s', A_t = a'}
\end{equation}
whenever $(s_{N(i)},a_{N(i)}) = (s'_{N(i)},a'_{N(i)})$.
\end{enumerate}
\label{assu:local}
\end{assu}

However, even if one-step rewards and transitions are local, under a stationary policy the stationary distribution of $S_t$ can have long-range correlations, because local interactions propagate over time. We therefore need a mixing assumption to ensure that an approximate local law is close to the true local law.

Formally, consider the utility function $U(\pi)$. Under Assumption \ref{assu:local}, with a global policy $\pi$,
\begin{equation}
U(\pi)=
\frac{1}{n}\sum_i\sum_{a_i,s_{N(i)}} v_i(a_i,s_{N(i)}) \Gamma_{i,\pi}(a_i,s_{N(i)}),
\end{equation}
where
\begin{equation}
\Gamma_{i,\pi}(a_i,s_{N(i)})
=\sum_{a_{-i},s_{-N(i)}} \gamma_\pi(a,s)
\end{equation}
Now consider a set of local policies $\cb{\tilde \pi_i}$ that sets
\begin{equation}
\tilde \pi_i(a_i\cond s_{N(i)}) = \EE{\pi(a_i\cond s_{N(i)},S_{-N(i)})}
\end{equation}
so that under $\gamma_\pi$ the conditional distribution of $A_i$ given $s_{N(i)}$ is the same under $\pi$ and $\tilde\pi$.
We need a mixing assumption under which
\begin{equation}
\Gamma_{i,\pi}(a_i,s_{N(i)}) \approx \Gamma_{i,\tilde\pi}(a_i,s_{N(i)})
\end{equation}
for all $i$, $a_i$, $s_{N(i)}$. Under this assumption, the value $U(\tilde \pi)$ is close to $U(\pi)$, so so restricting attention to local policies incurs only a small regret in the objective.
Similarly, using the same notation, we can write the IC constraints as
\begin{equation}
\sum_s v_i(a_i,s_{N(i)})\Gamma_{i,\pi}(a_i,s_{N(i)})
\ge \sum_s v_i(a_i',s_{N(i)})\Gamma_{i,\pi}(a_i,s_{N(i)}).
\end{equation}
Then, under the mixing assumption, and if $v_i$ is bounded a.s., both the objective and each side of the IC inequalities change only by a small amount when we replace the global policy $\pi$ by its local counterpart $\tilde\pi$. Thus restricting attention to local policies leads to at most a small loss in both feasibility and performance.\footnote{We also need to handle the stationarity constraint. This requires that the learned local measures are consistent with the local dynamics induced by $\pi$ and that, whenever two neighborhoods overlap, their marginals on the overlap coincide.}


\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
